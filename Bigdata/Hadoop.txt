Hadoop

#P1
##EASY
###BC
1.Which of the following commands will create a directory named "Big-data" in the user's HDFS home directory? (cosider the username is "ken").

A.hdfs dfs -mkdir Big-data
B.mkdir /user/ken/Big-data
C.hdfs dfs mkdir /user/ken/Big-data
D.hdfs -mkdir /user/ken/Big-data

Answer:A
Explanation:Using mmkdir-we can create the directory.In Hadoop dfs there is no home directory by default.

#P1
##EASY
###BC
2.Which of the following options are not describe hadoop?

A.Open source
B.Real-Time
C.Java-Based
D.Distributed computing approach

Answer:B

#P1
##EASY
###BC
3.Which of the following characteristics are not of Big data?

A.Volume
B.Variety
C.Value
D.Accuracy

Answer:D
Explanation:Volume,variety and value all three are the characteristics of Big data.

#P1
##EASY
###BC
4.When a jobTracker tries to find somewhere to schedules a task is first looks for

A.A node with empty slot in the same rack as datanode
B.Any node on the same rack as the datanode
C.Any node on the rack adjacent to rack of the datanode
D.Just any node in the cluster

Answer:A
Explanation:When the JobTracker tries to find somewhere to schedule a task within the MapReduce operations, it will looks for empty slot on the same server that hosts the DataNode or else it looks for an empty slot on a machine.

#P1
##EASY
###TF
5.State True/False: Many experts consider Hadoop is a substitute for the business data warehouse because it have an capacity for managing unstructured and semistructured data for handling constantly increasing data volumes.

A.True
B.False

Answer:B

#P1
##EASY
###BC
6.How will you create the directory "data" in the HDFS root directory?

A.hdfs dfs -mkdir /data  
B.hdfs dfs -mkdir data
C./data
D.dfs -mkdir /data

Answer:A

#P1
##EASY
###MA
7.Choose which of the following is a schedulers in YARN

A.FIFO scheduler
B.LIFO scheduler
C.Fair scheduler
D.None of the above

Answer:A and C
Explantion:In YARN there are three types of schedulers available.
1.FIFO
2.Capacity
3.Fair.

#P1
##EASY
###BC
8.The heartbeat signal are sent from

A.JObtracker to Tasktracker
B.Tasktracker to Job tracker
C.Tasktracker to namenode
D.Jobtracker to namenode

Answer:B
Explanation:A Heartbeat signal sent from Datanode to Namenode, it will indicate that Data node is alive.

#P1
##EASY
###TF
9.State True/False:Hadoop has two major layers processing layer and storage layer.

A.True
B.False

Answer:A
Explanation:Hadoop has two major layers one is processing layer(MapReduce) and another one is Storage layer(HDFS).

#P1
##EASY
###MA
10.Which of the following commands can be used to copy a file from the local machine to HDFS? 

A.hdfs dfs -copyFromLocal <source> <target on HDFS>
B.hdfs dfs -put <source> <target on HDFS>
C.hdfs dfs -cp <source> <target on HDFS>
D.hdfs dfs -mv <source> <target on HDFS>

Answer: A and B
Explanation: In order to copy a file from the local file system to HDFS, use hdfs dfs -put or -copyFromLocal command, specify the path of source file and then destination file path.

#P1
##EASY
###TF
11.State True/False:Hadoop can run in 5 different modes.

A.True
B.False

Answer:B
Explanation:Hadoop can run in 3 different types of modes.
1. Local distributed mode.
2. Single distributed mode.
3. Fully distributed mode.

#P2
##EASY
###BC
12.Choose the best one:Hadoop framework is written in which laguage?

A.Python
B.R
C.Java
D.Go

Answer:C
Explanation:Hadoop was written in Java because it was used to "fix" the problems.

#P1
##EASY
###BC
13.What is the fullform of Hadoop?

A.Highly Available distributes object oriented porgramming
B.Highly available Out Of Programming
C.High availability distributed object oriented programming
D.None of the above

Answer:C

#P1
##EASY
###TF
14.True/Fasle:In hadoop we can use an empty namenode in hdfs.

A.True
B.False

Answer:B
Explanation:There does not exist any NameNode without data. If it is a NameNode then it should have some sort of data in it.

#P2
##EASY
###BC
15.Which of the following is not deal with small file issue?

A.Hadoop archives
B.HBase
C.Sequence files
D.None of the above

Answer:D
Explanation:All of the above options are correct Because it is always deal with samll file issues.

#P1
##EASY
###BC
16.Which one is used to stores in each block of file into hdfs?

A.Name node
B.Data node
C.Master node
D.Nnoe of these

Answer:B
Explanation:In HDFS it divides the data into a blocks and each blocks are stored into the data node.

#P1
##EASY
###TF
17.State True/False:The default size of distributed cache is 15GB.

A.True
B.False

Answer:B
Explanation:The default size of the distributed cache is 10 GB only.

#P1
##EASY
###BC
18.Which of the following configuration file is used to control the HDFS replication factor?

A.mapred-site.xml
B.hdfs-site.xml
C.core-site.xml
D.yarn-site.xml

Answer:B

#P1
##EASY
###BC
19.What will be use of below command in hdfs?
hdfs dfs -mv  <src> <target>

A.This commands will trow an error.
B.It will move a file from the specified HDFS source folder to the target folder also on HDFS.  
C.It will move a file from the source folder on the local machine to the target folder on HDFS.
D.Both B and C

Answer:B

#P2
##EASY
###BC
20.Can we modify the files already in HDFS?

A.Yes,we can modify
B.It will throw an error
C.No,we can't modify
D.None of the above

Answer:C
Explanation:We can't modify the files already in hdfs because hdfs follows Write Once Read Many model.

#P1
##EASY
###BC
21.What is the full-form of YARN?

A.Yet Another Resource Negotiator.
B.Yet Another Resource Navigator.
C.Yet Another Resource Number.
D.Yet Another Resource Numerator.

Answer:A

#P1
##EASY
###BC
22.Choose the Which of the following are the core elements of YARN? 

A.Application Master
B.MapReduce
C.Hive
D.HDFS

Answer:A
Explanation:Except the application manager all other options are not the core elements of YARN.

#P2
##EASY
###BC
23.can best be described as a programming model used to develope Hadoop-based applications that can process massive amounts of data?

A.MapReduce
B.Mahout
C.oozie
D.All of the mentioned

Answer:A

#P2
##EASY
###BC
24.Which has the worldâ€™s largest Hadoop cluster?

A.Apple
B.Datamatics
C.Facebook   
D.None of the mentioned

Answer:C
Explanation:Except the Facebook all other clusters are not world's largest hadoop cluster.

#P2
##EASY
###BC
25.Which size of data is called as Big Data.

A.Meta
B.Peta
C.Giga
D.Tera

Answer:B

#P1
##EASY
###TF
26.YARN,HDFS and YARN are not the components of BigData.

A.True
B.False

Answer:B

#P1
##EASY
###BC
27.Which of the following does the Node Manager send back to the Resource Manager?

A.Heartbeat
B.Pulse
C.Poke
D.YARN

Answer:A

#P1
##EASY
###BC
28.Which of the following explains a Container in YARN?

A.A collection of a specfic set of resources.
B.Sets up the container environment.
C.Sends a heartbeat to the Resource Manager.
D.Is responsible for allocating resources to various running applications depending on the common constraints of capacities, queues, and so on.

Answer:A

#P1
##EASY
###BC
29.Which option can be provided to specify the number of reducers when running a MapReduce command on Hadoop?

A.-numReduceTasks
B.-reduceTasks
C.-tasks
D.-numReduce

Answer.A

#P1
##EASY
###TF
30.State True/False:KeyValueTextInputFormat breaks the line into key and value by a tab character. Here Key is everything up to the tab character,while the value is the remaining part of the line after tab character.

A.True
B.False

Answer:A

#P1
##EASY
###BC
31.Partitioner controls the partitioning of what kind of data?

A.final keys
B.final values
C.intermediate keys
D.intermediate values

Answer:C

#P1
##EASY
###BC
32.MapReduce splits the input data into smal blocks for the Mapper to process.What is the default max size of these blocks?

A.128MB
B.256MB
C.128Gb
D.300TB

Answer:A
Explanation:HDFS stores the file as blocks and each block is contain 128Mb in size. 
Mapreduce processes this HDFS file.
Each mapper processes a block. So, 128MB is a single block of size which will not be further split.

#P1
##EASY
###BC
33.How many active Namenodes can there be on a Hadoop Cluster?

A.10
B.As many as you want
C.20
D.None of the above

Answer:D
Explanation:In Hadoop cluster we have only 1 namenode can be active.

#P1
##EASY
###BC
34.If a 600 MB file is given as an input to a MapReduce process, in how many blocks will the file be splitted? And what will be the size of the last block?

A.5 and 88
B.5 and 78
C.4 and 88
D.6 and 88

Answer:A

#P1
##EASY
###TF
35.State True/False: TextInputFormat treats each line of each input file as a separate record and performs parsing.

A.True
B.False

Answer:False
Explanation: TextInputFormat treats each line of each input file as a separate record and performs no parsing.

#P1
##EASY
###TF
36.YARN acts like an Operating System to Hadoop.

A.True
B.False

Answer:True
Explanation:YARN is the resource management unit of Hadoop and acts like the OS to Hadoop.

#P2
##EASY
###BC
37.Suppose there is file of size 514 MB stored in HDFS (Hadoop 2.x) using default block size configuration and default replication factor. Then, how many blocks will be created in total?

A.4
B.5
C.15
D.None of the above

Answer:B
Explanation:A file of size 514 MB will be divided into 5 blocks ( 514 MB/128 MB) where the first four blocks will be of 128 MB and the last block will be of 2 MB only.

#P1
##EASY
###BC
38.Can you change the block size of HDFS files?

A.We can't change the block size
B.We can change
C.Using the cluster property
D.None of the above

Answer:B
Explanation:Yes, I can change the block size of HDFS files by changing the default size parameter present in hdfs-site.xml. But, I will have to restart the cluster for this property change to take effect.

#P2
##EASY
###BC
39.Which of the following is correct for Hadoop?

A.Namenode is the SPOF in Hadoop 1.x
B.Namenode is the SPOF in Hadoop 2.x
C.Namenode keeps an image of the filesystem
D.Both A and C

Answer:D

#P2
##EASY
###BC
40.Your clusterâ€™s HDFS block size in 64MB. You have directory containing 100 plain text files, each of which is 100MB in size. The InputFormat for your job is TextInputFormat. Determine how many Mappers will run?

A.64
B.100
C.200
D.640

Answer:C

#P2
##EASY
###BC
41.In a MapReduce job with 500 map tasks, how many map task attempts will there be

A.It depends on the number of reducer jobs
B.Between 400 and 500
C.At most 500
D.At least 500

Answer:D

#P2
##EASY
###TF
42.State True/False:We can modify the file present in HDFS.

A.True
B.False

Answer:False
Explanation:No, we can't modify the files already present in HDFS, as HDFS follows Write Once Read Many model.

#P1
##EASY
###TF
43.Standby namenode is a concept introduced in HDFS HA

A.True
B.False

Answer:A

#P1
##EASY
###BC
44.To perform local aggregation of the intermediate outputs, MapReduce users can optionally specify which object?

A.Reducer
B.Combiner
C.Mapper
D.Counter

Answer:B

#P1
##EASY
###BC
45.How would you see the running application in yarn from the command line ?

A.App -list
B.Yarn application -list
C.yarn application _list
D.None of the above

Answer:B

#P2
##EASY
###TF
46.State True/False:Multiple clients write into an HDFS file concurrently.

A.True
B.False

Answer:B
Explanation:No, multiple clients canâ€™t write into an HDFS file concurrently.

#P1
##EASY
###BC
47.The Hadoop framework provides a mechanism for coping with machine issues such as faulty configuration or impending hardware failure. MapReduce detects that one or a number of machines are performing poorly and starts another copy of a map or reduce task. All the tasks run simultaneously and the task finish first are used. This is called:

A.Combiner
B.IdentityMapper
C.IdentityReducer
D.Speculative Execution

Answer:D

#P2
##EASY
###BC
48.To verify job status, what are the things look for the values are? 

A.SUCCEEDED; syslog
B.SUCCEEDED; stdout
C.DONE; syslog
D.DONE; stdout

Answer:B
Explanation:To verify job status,look for the value succeeded in the stdout.

#P1
##EASY
###BC
49.To get the total number of mapped input records in a map job task, you should review the value of which counter?

A.FileInputFormatCounter
B.FileSystemCounter
C.JobCounter
D.TaskCounter 

Answer:D

#P2
##EASY
###BC
50.You have the following key-value pairs as output from your Map task: (the, 1) (fox, 1) (faster, 1) (than, 1) (the, 1) (dog, 1). How many keys will be passed to the Reducer's reduce method?

A.Five
B.Three
C.Four
D.Two

Answer:A

#P1
##EASY
###BC
51.Choose the correct answer:What are the primary phases of a Reducer?

A.combine, map, and reduce
B.shuffle, sort, and reduce
C.reduce, sort, and combine
D.map, sort, and combine

Answer:B
Explanation:Shuffle,sort and reduce are the primary phases of the reducer.

#P2
##EASY
###BC
52.For high availability, use multiple nodes of which type?

A.data
B.name
C.memory
D.worker

Answer:B

#P1
##EASY
###BC
53.DataNode supports which type of drives?

A.hot swappable
B.cold swappable
C.warm swappable
D.non-swappable

Answer:A
Explanation:Datanode supports the hot swappable drives. The user can add or replace volume of data from HDFS without shutting down the DataNode.

#P1
##EASY
###BC
54.In a MapReduce job, where does the map() function run?

A.on the reducer nodes of the cluster
B.on the data nodes of the cluster 
C.on the master node of the cluster
D.on every node of the cluster

Answer:B

#P1
##EASY
###BC
55.To reference a master file for lookups during Mapping, what type of cache should be used?

A.distributed cache
B.local cache
C.partitioned cache
D.cluster cache

Answer:A

#P2
##EASY
###BC
56.In what form is Reducer output presented?

A.compressed
B.sorted
C.not sorted
D.encrypted

Answer:A

#P1
##EASY
###BC
57.Which library should be used to unit test MapReduce code?

A.JUnit
B.XUnit
C.MRUnit
D.HadoopUnit

Answer:C
Answer:MRUnit is a JUnit-based Java library that allows us to unit test Hadoop MapReduce programs.

#P1
##EASY
###BC
58.If you started the NameNode, then which kind of user must you be?

A.hadoop-user
B.super-user
C.node-user
D.admin-user

Answer:B
Explanation:If you started the NameNode, then you are the super-user. The super-user can do anything in that permissions checks never fail for the super-user.

#P2
##EASY
###BC
59.To create a MapReduce job, what should be coded first?

A.a static job() method
B.a Job class and instance
C.a job() method
D.a static Job class

Answer:B

#P1
##EASY
###BC
60.What is the bare-minimum of testing needed within MR Unit for any Map Reduce job.

A.No tests are needed, Hadoop will handle all issues encountered.
B.Only test the Reducer for performance.
C.Only one test for the MapReduce combination.
D.A Test for the Mapper, a test for the Reducer, and a test for the MapReduce combination.

Answer:D

#P1
##EASY
###BC
61.What is the purpose of a Combiner?

A.used to combine data.
B.used to map the data further more.
C.used to partition the data into smaller key/value pairs.
D.used to act as an intermediate Reducer.

Answer:D
Explanation:Combiner is act as an intermediate reducer.

#P1
##EASY
###TF
62.State True/False:hdfs-site.xml informs Hadoop daemon where NameNode runs on the cluster.

A.True
B.False

Answer:B
Explanation:hdfs-site.xml contains configuration settings of HDFS daemons.

#P1
##EASY
###MA
63.What are the main hadoop configuration files?

A.core-site.xml
B.hdfs-site.xml
C.mapred-site.xlm
D.hadoop-site.xml

Answer:A and B
Explanation:The 4 main hadoop configuration files are :
1. core-site.xml
2. hdfs-site.xml
3. mapred-site.xml 
4. yarn-site.xml.

#P1
##EASY
###BC
64.State True/False:The hdfs-site.xml contains the configuration settings for HDFS daemons.

A.True
B.False

Answer:A

#P1
##EASY
###BC
65.What is the default replication factor in HDFS for non-pseudo distributed clusters?

A.2
B.1
C.3
D.4

Answer:C

#P1
##EASY
###BC
66.Can we use an empty namenode in hdfs?

A.Yes
B.No
C.using exist any namenode 
D.None of the above

Answer:B
Explanation:No we can't able to use an empty naemnode in hdfs.
There does not exist any NameNode without data. If it is a NameNode then it should have some sort of data in it.

#P1
##EASY
###TF
67.HDFS file formats supported are Json, Avro and Parquet.

A.True
B.False

Answer:A

#P1
##EASY
###TF
68.The default replication factor is 31.

A.True
B.False

Answer:False
Explanation:The dafault replication factor is 3.

#P1
##EASY
###TF
69.Default Replication Factor could not be changed in hdfs.

A.True
B.False

Answer:B
Explanation:In hdfs we can't change the default replication factor.We can change the default replication factor in three ways.

#P1
##EASY
###MA
70.Choose which one is the correct way of changing the default replication factor in hdfs.

A.hadoop fs â€“setrep â€“w 3 -R /directory_location
B.haDoop Fs -setreps -w 3 -R /directory_location
C.hadoop fs â€“setrep â€“w 3 /file_location
D.None of the above

Answer:A and C

#P1
##EASY
###BC
71.What is the full form of fsck?

A.file system check
B.file system control
C.file system checker
D.file system consistant

Answer:A
Explanation:It is designed for reporting the problems with the files in HDFS.

#P1
##EASY
###BC
72.What happens if you get a 'connection refused java exception' when you type hadoop fsck /?

A.DataNode is not working
B.Both NameNode and DataNode is not working
C.NameNode is not working
D.None of the above

Answer:C

#P2
##EASY
###TF
73.State True/False:We can view compressed files in HDFS using hadoop hdfs -text /filename command.

A.True
B.False

Answer:B
Explanation:We can view compressed files in HDFS using hadoop fs -text /filename command.

#P1
##EASY
###BC
74.What is the use of this command "hdfs dfsadmin -safemode get".

A.used to check the status of the unknown mode.
B.used to enter into the safe mode.
C.used to know the status of safe mode.
D.All of the above

Answer:C

#P1
##EASY
###TF
75.State True/False:jps command is used to check all the Hadoop daemons are running on the machine.

A.True
B.False

Answer:True

#P1
##EASY
###BC
76.With which command can you copy a file existing in HDFS to your local file system?

A.hdfs dfs -copyFromLocal localLocation remoteLocation
B.hdfs dfs -copyToLocal localLocation remoteLocation
C.hdfs dfs -copyFromLocal remoteLocation localLocation
D.hdfs dfs -copyToLocal remoteLocation localLocation

Answer:C

#P1
##EASY
###BC
77.What is the default HDFS block size in hadoop 2.x?

A.128GB
B.64MB
C.128MB
D.120MB

Answer:C

#P1
##EASY
###BC
78.What does masters file consist of?

A.NameNode
B.DataNode
C.Secondary NameNode
D.Both DataNode and NameNode

Answer:C
Explanation:The masters file contains Secondary NameNode server location.

#P2
##EASY
###BC
79.What is the output of the Reducer?

A.A relational table
B.An update to the input file
C.A single,combined list
D.A set of key and value pairs

Answer:D
Explanation:The output of the reducer is the final output a set of key and value pairs which is stored in HDFS.

#P1
##EASY
###BC
80.NameNode metadata is finally stored in?

A.Edit Log
B.FSimage
C.Log File
D.Update Log

Answer:B
Explanation:NameNode metadata is finally stored in the FSimage.

#P1
##EASY
###BC
81.What will hadoop do if the task fails more than four (By default ) times?

A.It will create again the same node
B.It will restart
C.It will kill the job
D.None of the above

Answer:C
Explanation:If the task fails, it will restart the task on some other TaskTracker and if the task fails more than four (By default ) times it will kill the job.

#P2
##EASY
###BC
82.Which is the used to write SQL queries without MapReduce?

A.Hive
B.Impala
C.Pig
D.None of the options

Answer:B

#P1
##EASY
###BC
83.Commodity hardware in Hadoop refers to?

A.All the Datanode should be of the Same procesing power and storage
B.Namenode can use any type of machine
C.Datanode can use any type of machine
D.None of the options

Answer:C

#P2
##EASY
###TF
84.State True/False:If the default replication factor is 1,Hadoop can't do parallel processing.

A.True
B.False

Answer:B
Explanation:Hadoop can do parallel processing when the default replication factor is 1.

#P1
##EASY
###TF
85.In Hadoop MapReduce, it is possible to support multiple reduce methods for the same Mapper input.

A.True
B.False

Answer:A
Explanation:yes you can do it - but it may be a pretty bad idea.

#P1
##EASY
###MA
86.Choose the correct answer based on the given statement:What will happen suppose a NameNode is failed.

A.The whole Hadoop cluster will not work.
B.No data loss  
C.The cluster work will be shut down
D.None of the above

Answer:A,B and C

#P2
##EASY
###BC
87.Where is HDFS replication factor controlled?

A.mapred-site.xml
B.yarn-site.xml
C.core-site.xml
D.hdfs-site.xml

Answer:D

#P1
##EASY
###TF
88.The intermediate output from a mapper is stored on HDFS

A.True
B.False

Answer:B


#P1
##EASY
###BC	
89.When is the earliest point at which the reduce method of a given Reducer can be called?

A.As soon as at least one mapper has finished processing its input split.
B.As soon as a mapper has emitted at least one record.
C.Not until all mappers have finished processing all records.
D.It depends on the inputformat used for the job.

Answer:C

#P1
##EASY
###BC
90.How many daemons are required to run a hadoop cluster?

A.6
B.2
C.1
D.5

Answer:D
Explanation:In Hadoop 5 daemons are required to run a hadoop cluster.
1. NameNode
2. DataNode
3. Secondary NameNode
4. JobTracker
5. TaskTracker.

#P2
##EASY
###TF
91.State True/False:It is possible to the copy files across the multiple Hadoop clusters.

A.True
B.False

Answer:A

#P1
##EASY
###BC
92.Does Hadoop replace data warehousing systems?

A.It is possible
B.It is not possible
C.Replace with datawarehouse system
D.None of the above

Answer:B
Explanation:Hadoop will not replace a data warehouse because the data and its platform are two non-equivalent layers in Data warehouse architecture.

#P1
##EASY
###BC
93.How will you skip the bad records in Hadoop?

A.skip record
B.Skip Records
C.SkipBadRecords class
D.skip bad record

Answer:C
Explanation:With the help of SkipBadRecords class , we can skip the bad records in hadoop.

#P1
##EASY
###BC
94.Which command is used to format the NameNode?

A.hdfs namenodes -format
B.hdfs node -remove
C.hdfs namenode -format
D.None of the above

Answer:C

95.If we want to copy 10 blocks from one machine to another, but another machine can copy only 8.5 blocks, can the blocks be broken at the time of replication?

A.Can be broke
B.Can be broke into 5 blocks
C.Can't be broken 
D.None of the above

Answer:C
Explanation:Before copying the blocks from one machine to another, the Master node will figure out what is the actual amount of space required, how many block are being used, how much space is available, and it will allocate the blocks accordingly.

#P2
##EASY
###BC
96.Hadoop 2.x and later implement which service as the resource coordinator?

A.kubernetes
B.JobManager
C.JobTracker
D.YARN

Answer:D

#P1
##EASY
###MA
97.What are Correct statements of pseudo mode?

A.Developement
B.Testing
C.In this mode all the daemons are run on the same machine
D.Analysing

Answer:A,B and C

#P1
##EASY
###TF
98.State True/False:This below command is used to enter into the safe mode.
hdfs dfsadmin -safemode leave

A.True
B.False

Answer:B
Explanation:The above is used toexit form the safe mode.

#P1
##EASY
###BC
99.Which of the following daemon is responsible for replication of data in hadoop?

A.DataNode
B.NameNode
C.Secondary NameNode
D.JobTracker

Answer:B
Explanation:In hadoop NameNode is is responsible for replication of data in hadoop.

#P1
##EASY
###TF
100.State True/False:In Hadoop the JobTracker is the service within Hadoop that runs MapReduce jobs on the cluster.

A.True
B.False

Answer:A

#P1
##EASY
###BC
101.Consider we have 3 files of size 64K, 65Mb and 127Mb. How many input splits will be made by Hadoop framework?

A.6 splits
B.8 splits
C.5 splits
D.7 splits

Answer:C
Explanation:Hadoop will make 5 splits because HDFS block size is 64 MB.
1 split for 64K files
2 splits for 65MB files
2 splits for 127MB files

#P1
##EASY
###BC
102.Which of the following interface needs to be implemented to create a mapper for the Hadoop?

A.orf.apache.hadoop.mapreduce.Mapper
B.org.apache.hadoop.mapreduce.Mapp
C.org.apache.hadoop.mapreduce.Mapper
D.org.apache.hadoops.mapreduce.Mappers

Answer:C

#P2
##EASY
###MA
103.Choose the correct statements what will happen when a datanode fails?

A.Jobtracker and namenode detect the failure
B.On the failed node all tasks are re-scheduled
C.Namenode replicates the users data to another node
D.None of the above

Answer:A,B & C

#P2
##EASY
###TF
104.State True/Faslse:fsimage file it doesn't keeps track of the latest checkpoint of the namespace.

A.True
B.False

Answer:B
Explanation:The fsimage file it keeps track of the latest checkpoint of the namespace.

#P1
##EASY
###BC
105.what is the port number for Task Tracker and Job Tracker?

A.50060  and 50070
B.50006 and 50007
C.50070 and 50003
D.50060 and 50030

Answer:D
Explanation:The port number for the task tracker and job tracker.

#P1
##EASY
###BC
106.What happens when a user submits a Hadoop job when the NameNode
is down? - does the job get in to hold or does it fail.

A.Job get in to hold
B.Job fails
C.Job succeed
D.None of the above

Answer:B
Explanation:The Hadoop job fails when the NameNode is down.

#P1
##EASY
###TF
107.Find the statements is True/False:The Hadoop job fails when the NameNode is down.

A.True
B.False

Answer:A

#P1
##EASY
###MA
108.What are the core methods of a reducer?

A.setup()
B.reduce()
C.mapper()
D.cleanup()

Answer:A,B & D

#P1
##EASY
###TF
109.Is it Release 2.7.1 version is the stable versions of Hadoop?

A.True
B.False

Answer:A

#P1
##EASY
###TF
110.State True/False:YARN is a replacement of Hadoop MapReduce?

A.True
B.False

Answer:B
Explanation:YARN is not a replacement of MapReduce in Hadoop but it is more powerful and efficient
technology that do all the resource managing in Hadoop 2.0 instead of MapReduce.

#P1
##EASY
###BC
111.Where would you configure the size of a block in a Hadoop environment?

A.dfs.block.size in hdfs-site.xmls
B.orc.write.variable.length.blocks in hive-default.xml
C.mapreduce.job.ubertask.maxbytes in mapred-site.xml
D.hdfs.block.size in hdfs-site.xml

Answer:A

#P2
##EASY
###BC
112.What is the datasize of MapReduce?

A.Petabytes
B.Gigabytes
C.Nano 
D.Anysize

Answer:A

#P1
##EASY
###TF
113.The name Hadoop is an acronym?

A.TRUE
B.FALSE

Answer:B

#P1
##EASY
###TF
114.State True/False:The right number of reduces to be 1.75.

A.True
B.False

Answer:A

#P2
##EASY
###BC
115.Which is used to help the not sorted in the Mapreduce framework for Hadoop?

A.Mapper
B.Cascader
C.Scalding
D.None of the mentioned

Answer:D
Explanation:The output of the reduce task is typically written to the FileSystem. The output of the Reducer is not sorted.   









